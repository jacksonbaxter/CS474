{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/porterjenkins/byu-cs474/blob/master/lab6_regularization.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yetiZVaizxbx"
   },
   "source": [
    "# Lab 6: Regularization\n",
    "\n",
    "Be sure and fill in all the cells marked a TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vBxUrtgz7GT"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "In this lab we will study the effect of regularization on training neural networks. Specifically, we will study how L2 regularization (also commonly referred to as weight decay) can improve the generalization ability of neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHaH75FB0fi1"
   },
   "source": [
    "## Deliverable\n",
    "\n",
    "You will turn in a completed version of notebook to Canvas/Learning Suite.  In various places you will see the words \"TO DO\". Follow the instructions at these places and write code to complete the instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_4_g5ETm53F"
   },
   "source": [
    "## Note\n",
    "For Q2 of the lab, you will want to make sure your notebook is connected to a GPU instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Qal-zKd0vYL"
   },
   "source": [
    "## Q1) L2 Regularization\n",
    "This question investigates adding L2 regularization to the loss function for the Gabor model as in figure 9.1 of Prince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9URmoNcd03XO"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ijb8rbcI1R4A"
   },
   "source": [
    "Let's create our training data 30 pairs `{x_i, y_i}`. We'll try to fit the Gabor model to these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lTTkOid1Fzu"
   },
   "outputs": [],
   "source": [
    "data = np.array([[-1.920e+00,-1.422e+01,1.490e+00,-1.940e+00,-2.389e+00,-5.090e+00,\n",
    "                 -8.861e+00,3.578e+00,-6.010e+00,-6.995e+00,3.634e+00,8.743e-01,\n",
    "                 -1.096e+01,4.073e-01,-9.467e+00,8.560e+00,1.062e+01,-1.729e-01,\n",
    "                  1.040e+01,-1.261e+01,1.574e-01,-1.304e+01,-2.156e+00,-1.210e+01,\n",
    "                 -1.119e+01,2.902e+00,-8.220e+00,-1.179e+01,-8.391e+00,-4.505e+00],\n",
    "                  [-1.051e+00,-2.482e-02,8.896e-01,-4.943e-01,-9.371e-01,4.306e-01,\n",
    "                  9.577e-03,-7.944e-02 ,1.624e-01,-2.682e-01,-3.129e-01,8.303e-01,\n",
    "                  -2.365e-02,5.098e-01,-2.777e-01,3.367e-01,1.927e-01,-2.222e-01,\n",
    "                  6.352e-02,6.888e-03,3.224e-02,1.091e-02,-5.706e-01,-5.258e-02,\n",
    "                  -3.666e-02,1.709e-01,-4.805e-02,2.008e-01,-1.904e-01,5.952e-01]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGCYSirE8RpN"
   },
   "source": [
    "### Part 1.a) Setting up the Gabor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvvB7RxE1WFj"
   },
   "source": [
    "Gabor model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEvSGoWZ1G03"
   },
   "outputs": [],
   "source": [
    "\n",
    "def model(phi,x):\n",
    "  sin_component = np.sin(phi[0] + 0.06 * phi[1] * x)\n",
    "  gauss_component = np.exp(-(phi[0] + 0.06 * phi[1] * x) * (phi[0] + 0.06 * phi[1] * x) / 32)\n",
    "  y_pred= sin_component * gauss_component\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrViC-u91Y3r"
   },
   "source": [
    "Let's define a function to plot the data, the model and its outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BUGdbZK1IbK"
   },
   "outputs": [],
   "source": [
    "def draw_model(data,model,phi,title=None):\n",
    "  x_model = np.arange(-15,15,0.1)\n",
    "  y_model = model(phi,x_model)\n",
    "\n",
    "  fix, ax = plt.subplots()\n",
    "  ax.plot(data[0,:],data[1,:],'bo')\n",
    "  ax.plot(x_model,y_model,'m-')\n",
    "  ax.set_xlim([-15,15]);ax.set_ylim([-1,1])\n",
    "  ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "  if title is not None:\n",
    "    ax.set_title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6z9koFC8IFP"
   },
   "source": [
    "Initialize the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plnyzQGI1JVo"
   },
   "outputs": [],
   "source": [
    "phi = np.zeros((2,1))\n",
    "phi[0] =  -5     # Horizontal offset\n",
    "phi[1] =  25     # Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3ozgyYc1k_t"
   },
   "source": [
    "Draw the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fG_N_eIb1kY0"
   },
   "outputs": [],
   "source": [
    "draw_model(data,model,phi, \"Initial parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPxEqrXu2LBp"
   },
   "source": [
    "Now let's compute the sum of squares loss for the training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wooOjHcO5B3N"
   },
   "source": [
    "**TODO:** Implement the sum of squares loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AAquhYW1LHT"
   },
   "outputs": [],
   "source": [
    "def compute_loss(data_x, data_y, model, phi):\n",
    "  pred_y = model(phi, data_x)\n",
    "  # TODO: add code here to compute the loss\n",
    "  loss = None\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKWEqtP42URR"
   },
   "source": [
    "Now let's plot the whole **loss function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nc7u9rMb1M8B"
   },
   "outputs": [],
   "source": [
    "# Define pretty colormap\n",
    "my_colormap_vals_hex =('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', '320f0a', '330f0b', '34100b', '35110c', '36110d', '37120e', '38120f', '39130f', '3a1410', '3b1411', '3c1511', '3d1612', '3e1613', '3f1713', '401714', '411814', '421915', '431915', '451a16', '461b16', '471b17', '481c17', '491d18', '4a1d18', '4b1e19', '4c1f19', '4d1f1a', '4e201b', '50211b', '51211c', '52221c', '53231d', '54231d', '55241e', '56251e', '57261f', '58261f', '592720', '5b2821', '5c2821', '5d2922', '5e2a22', '5f2b23', '602b23', '612c24', '622d25', '632e25', '652e26', '662f26', '673027', '683027', '693128', '6a3229', '6b3329', '6c342a', '6d342a', '6f352b', '70362c', '71372c', '72372d', '73382e', '74392e', '753a2f', '763a2f', '773b30', '783c31', '7a3d31', '7b3e32', '7c3e33', '7d3f33', '7e4034', '7f4134', '804235', '814236', '824336', '834437', '854538', '864638', '874739', '88473a', '89483a', '8a493b', '8b4a3c', '8c4b3c', '8d4c3d', '8e4c3e', '8f4d3f', '904e3f', '924f40', '935041', '945141', '955242', '965343', '975343', '985444', '995545', '9a5646', '9b5746', '9c5847', '9d5948', '9e5a49', '9f5a49', 'a05b4a', 'a15c4b', 'a35d4b', 'a45e4c', 'a55f4d', 'a6604e', 'a7614e', 'a8624f', 'a96350', 'aa6451', 'ab6552', 'ac6552', 'ad6653', 'ae6754', 'af6855', 'b06955', 'b16a56', 'b26b57', 'b36c58', 'b46d59', 'b56e59', 'b66f5a', 'b7705b', 'b8715c', 'b9725d', 'ba735d', 'bb745e', 'bc755f', 'bd7660', 'be7761', 'bf7862', 'c07962', 'c17a63', 'c27b64', 'c27c65', 'c37d66', 'c47e67', 'c57f68', 'c68068', 'c78169', 'c8826a', 'c9836b', 'ca846c', 'cb856d', 'cc866e', 'cd876f', 'ce886f', 'ce8970', 'cf8a71', 'd08b72', 'd18c73', 'd28d74', 'd38e75', 'd48f76', 'd59077', 'd59178', 'd69279', 'd7937a', 'd8957b', 'd9967b', 'da977c', 'da987d', 'db997e', 'dc9a7f', 'dd9b80', 'de9c81', 'de9d82', 'df9e83', 'e09f84', 'e1a185', 'e2a286', 'e2a387', 'e3a488', 'e4a589', 'e5a68a', 'e5a78b', 'e6a88c', 'e7aa8d', 'e7ab8e', 'e8ac8f', 'e9ad90', 'eaae91', 'eaaf92', 'ebb093', 'ecb295', 'ecb396', 'edb497', 'eeb598', 'eeb699', 'efb79a', 'efb99b', 'f0ba9c', 'f1bb9d', 'f1bc9e', 'f2bd9f', 'f2bfa1', 'f3c0a2', 'f3c1a3', 'f4c2a4', 'f5c3a5', 'f5c5a6', 'f6c6a7', 'f6c7a8', 'f7c8aa', 'f7c9ab', 'f8cbac', 'f8ccad', 'f8cdae', 'f9ceb0', 'f9d0b1', 'fad1b2', 'fad2b3', 'fbd3b4', 'fbd5b6', 'fbd6b7', 'fcd7b8', 'fcd8b9', 'fcdaba', 'fddbbc', 'fddcbd', 'fddebe', 'fddfbf', 'fee0c1', 'fee1c2', 'fee3c3', 'fee4c5', 'ffe5c6', 'ffe7c7', 'ffe8c9', 'ffe9ca', 'ffebcb', 'ffeccd', 'ffedce', 'ffefcf', 'fff0d1', 'fff2d2', 'fff3d3', 'fff4d5', 'fff6d6', 'fff7d8', 'fff8d9', 'fffada', 'fffbdc', 'fffcdd', 'fffedf', 'ffffe0')\n",
    "my_colormap_vals_dec = np.array([int(element,base=16) for element in my_colormap_vals_hex])\n",
    "r = np.floor(my_colormap_vals_dec/(256*256))\n",
    "g = np.floor((my_colormap_vals_dec - r *256 *256)/256)\n",
    "b = np.floor(my_colormap_vals_dec - r * 256 *256 - g * 256)\n",
    "my_colormap = ListedColormap(np.vstack((r,g,b)).transpose()/255.0)\n",
    "\n",
    "def draw_loss_function(compute_loss, data,  model, my_colormap, phi_iters = None):\n",
    "\n",
    "  # Make grid of offset/frequency values to plot\n",
    "  offsets_mesh, freqs_mesh = np.meshgrid(np.arange(-10,10.0,0.1), np.arange(2.5,22.5,0.1))\n",
    "  loss_mesh = np.zeros_like(freqs_mesh)\n",
    "  # Compute loss for every set of parameters\n",
    "  for idslope, slope in np.ndenumerate(freqs_mesh):\n",
    "     loss_mesh[idslope] = compute_loss(data[0,:], data[1,:], model, np.array([[offsets_mesh[idslope]], [slope]]))\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  fig.set_size_inches(8,8)\n",
    "  ax.contourf(offsets_mesh,freqs_mesh,loss_mesh,256,cmap=my_colormap)\n",
    "  ax.contour(offsets_mesh,freqs_mesh,loss_mesh,20,colors=['#80808080'])\n",
    "  if phi_iters is not None:\n",
    "    ax.plot(phi_iters[0,:], phi_iters[1,:],'go-')\n",
    "  ax.set_ylim([2.5,22.5])\n",
    "  ax.set_xlabel('Offset $\\phi_{0}$'); ax.set_ylabel('Frequency, $\\phi_{1}$')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTYEm8Ye2ZU0"
   },
   "outputs": [],
   "source": [
    "draw_loss_function(compute_loss, data, model, my_colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wyHl9LQ2o0u"
   },
   "source": [
    "Now let's compute the gradient vector for a given set of parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol\\phi} = \\begin{bmatrix}\\frac{\\partial L}{\\partial \\phi_0} \\\\\\frac{\\partial L}{\\partial \\phi_1} \\end{bmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpKRhFI13GKT"
   },
   "source": [
    "The Gabor model has two parameters, $\\phi_0$ and $\\phi_1$. We need to comptue the deriviate of the loss (sum of squares) with respect both quantities. Below we define two functions that each implement of the two partial deriviates we need. This is a hassle to get right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asjzLQ212pCy"
   },
   "outputs": [],
   "source": [
    "def gabor_deriv_phi0(data_x,data_y,phi0, phi1):\n",
    "    x = 0.06 * phi1 * data_x + phi0\n",
    "    y = data_y\n",
    "    cos_component = np.cos(x)\n",
    "    sin_component = np.sin(x)\n",
    "    gauss_component = np.exp(-0.5 * x *x / 16)\n",
    "    deriv = cos_component * gauss_component - sin_component * gauss_component * x / 16\n",
    "    deriv = 2* deriv * (sin_component * gauss_component - y)\n",
    "    return np.sum(deriv)\n",
    "\n",
    "def gabor_deriv_phi1(data_x, data_y,phi0, phi1):\n",
    "    x = 0.06 * phi1 * data_x + phi0\n",
    "    y = data_y\n",
    "    cos_component = np.cos(x)\n",
    "    sin_component = np.sin(x)\n",
    "    gauss_component = np.exp(-0.5 * x *x / 16)\n",
    "    deriv = 0.06 * data_x * cos_component * gauss_component - 0.06 * data_x*sin_component * gauss_component * x / 16\n",
    "    deriv = 2*deriv * (sin_component * gauss_component - y)\n",
    "    return np.sum(deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KU4AIlfm4CId"
   },
   "source": [
    "Now, let's implement a function that puts each partial deriviate into a vector, called a gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpJQ9qN73sfH"
   },
   "outputs": [],
   "source": [
    "def compute_gradient(data_x, data_y, phi):\n",
    "    dl_dphi0 = gabor_deriv_phi0(data_x, data_y, phi[0],phi[1])\n",
    "    dl_dphi1 = gabor_deriv_phi1(data_x, data_y, phi[0],phi[1])\n",
    "    # Return the gradient\n",
    "    return np.array([[dl_dphi0],[dl_dphi1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msrupp4A4JTx"
   },
   "source": [
    "Now we are ready to find the minimum.  For simplicity, we'll just use regular (non-stochastic) gradient descent with a fixed learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJLIml8g4nfn"
   },
   "source": [
    "**TODO:** Implement the gradient descent updates in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl2sByDM4KNw"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_step(phi, data):\n",
    "  # TODO: Implement the gradient descent step\n",
    "  # Step 1:  Compute the gradient\n",
    "  # Step 2:  Update the parameters -- note we want to search in the negative (downhill direction)\n",
    "  alpha = 0.1 # learning rate\n",
    "  phi = None\n",
    "  return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DreplmlR4Lys"
   },
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "n_steps = 41\n",
    "phi_all = np.zeros((2,n_steps+1))\n",
    "phi_all[0,0] = 2.6\n",
    "phi_all[1,0] = 8.5\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
    "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
    "\n",
    "for c_step in range (n_steps):\n",
    "  # Do gradient descent step\n",
    "  phi_all[:,c_step+1:c_step+2] = gradient_descent_step(phi_all[:,c_step:c_step+1],data)\n",
    "  # Measure loss and draw model every 8th step\n",
    "  if c_step % 8 == 0:\n",
    "    loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
    "    draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
    "\n",
    "draw_loss_function(compute_loss, data, model, my_colormap, phi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aILeWFao-w7d"
   },
   "source": [
    "## Part 1.b) Applying L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2BgidJ442BE"
   },
   "source": [
    "Unfortunately, when we start from this position, the solution descends to a local minimum and the final model doesn't fit well.<br><br>\n",
    "\n",
    "But what if we had some weak knowledge that the solution was in the vicinity of $\\phi_0=0.0$, $\\phi_{1} = 12.5$ (the center of the plot)?\n",
    "\n",
    "Let's add a term to the loss function that penalizes solutions that deviate from this point.  \n",
    "\n",
    "\\begin{equation}\n",
    "L'[\\boldsymbol\\phi] = L[\\boldsymbol\\phi]+ \\lambda\\cdot \\Bigl(\\phi_{0}^2+(\\phi_1-12.5)^2\\Bigr)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ controls the relative importance of the original loss and the regularization term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXVPqJjm5064"
   },
   "source": [
    "**TODO**: define a function that implements the L2 regularization term (term in large parentheses in the above equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C5z33_IM420e"
   },
   "outputs": [],
   "source": [
    "# Computes the regularization term\n",
    "def compute_reg_term(phi0,phi1):\n",
    "  # TODO Replace this line\n",
    "  reg_term = None\n",
    "  return reg_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgNdJMtT6NcP"
   },
   "source": [
    "**TODO**: Now define the whole loss function, which is the sum of squares plus $\\lambda$ times the L2 regularizatoin term, or $L'[\\phi]$ defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGw2OS5W6Khd"
   },
   "outputs": [],
   "source": [
    "# Note I called the weighting lambda_ to avoid confusing it with python lambda functions\n",
    "def compute_loss2(data_x, data_y, model, phi, lambda_):\n",
    "  pred_y = model(phi, data_x)\n",
    "  # TODO: define the loss function\n",
    "  loss = None\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q7O7TK470qw"
   },
   "source": [
    "Code to draw the regularization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miVbk2nk6ep7"
   },
   "outputs": [],
   "source": [
    "def draw_reg_function():\n",
    "\n",
    "  # Make grid of offset/frequency values to plot\n",
    "  offsets_mesh, freqs_mesh = np.meshgrid(np.arange(-10,10.0,0.1), np.arange(2.5,22.5,0.1))\n",
    "  loss_mesh = np.zeros_like(freqs_mesh)\n",
    "  # Compute loss for every set of parameters\n",
    "  for idslope, slope in np.ndenumerate(freqs_mesh):\n",
    "     loss_mesh[idslope] = compute_reg_term(offsets_mesh[idslope], slope)\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  fig.set_size_inches(8,8)\n",
    "  ax.contourf(offsets_mesh,freqs_mesh,loss_mesh,256,cmap=my_colormap)\n",
    "  ax.contour(offsets_mesh,freqs_mesh,loss_mesh,20,colors=['#80808080'])\n",
    "  ax.set_ylim([2.5,22.5])\n",
    "  ax.set_xlabel('Offset $\\phi_{0}$'); ax.set_ylabel('Frequency, $\\phi_{1}$')\n",
    "  plt.show()\n",
    "\n",
    "# Draw the regularization function.  It should look similar to figure 9.1b\n",
    "draw_reg_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PGU_6IL78UW"
   },
   "source": [
    "Code to draw loss function with regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQpA87Rd8A36"
   },
   "outputs": [],
   "source": [
    "def draw_loss_function_reg(data,  model, lambda_, my_colormap, phi_iters = None):\n",
    "\n",
    "  # Make grid of offset/frequency values to plot\n",
    "  offsets_mesh, freqs_mesh = np.meshgrid(np.arange(-10,10.0,0.1), np.arange(2.5,22.5,0.1))\n",
    "  loss_mesh = np.zeros_like(freqs_mesh)\n",
    "  # Compute loss for every set of parameters\n",
    "  for idslope, slope in np.ndenumerate(freqs_mesh):\n",
    "     loss_mesh[idslope] = compute_loss2(data[0,:], data[1,:], model, np.array([[offsets_mesh[idslope]], [slope]]), lambda_)\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  fig.set_size_inches(8,8)\n",
    "  ax.contourf(offsets_mesh,freqs_mesh,loss_mesh,256,cmap=my_colormap)\n",
    "  ax.contour(offsets_mesh,freqs_mesh,loss_mesh,20,colors=['#80808080'])\n",
    "  if phi_iters is not None:\n",
    "    ax.plot(phi_iters[0,:], phi_iters[1,:],'go-')\n",
    "  ax.set_ylim([2.5,22.5])\n",
    "  ax.set_xlabel('Offset $\\phi_{0}$'); ax.set_ylabel('Frequency, $\\phi_{1}$')\n",
    "  plt.show()\n",
    "\n",
    "# This should look something like figure 9.1c\n",
    "draw_loss_function_reg(data, model, 0.2, my_colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeenUPxu-5jL"
   },
   "source": [
    "Now we'll compute the derivatives $\\frac{\\partial L'}{\\partial\\phi_0}$ and $\\frac{\\partial L'}{\\partial\\phi_1}$ of the regularized loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "L'[\\boldsymbol\\phi] = L[\\boldsymbol\\phi]+ \\lambda\\cdot \\Bigl(\\phi_{0}^2+(\\phi_1-12.5)^2\\Bigr)\n",
    "\\end{equation}\n",
    "\n",
    "so that we can perform gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIqB70wT-8LS"
   },
   "source": [
    "**TODO:** Implement the partial deriviates each the regularized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYQuYgZL_dZm"
   },
   "outputs": [],
   "source": [
    "def compute_gradient2(data_x, data_y, phi, lambda_):\n",
    "    # TODO: compute partial derivates of each phi here:\n",
    "    dl_dphi0 = None\n",
    "    dl_dphi1 = None\n",
    "    # Return the gradient\n",
    "    return np.array([dl_dphi0, dl_dphi1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23QD6AqLtIPw"
   },
   "source": [
    "**TODO:** Implement the gradient descent step with the regularized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiKlRlz8tNWZ"
   },
   "outputs": [],
   "source": [
    "def gradient_descent_step2(phi, lambda_, data):\n",
    "  # TODO: Implement gradient descent\n",
    "  # Step 1:  Compute the gradient\n",
    "  gradient = None\n",
    "  # Step 2:  Update the parameters -- note we want to search in the negative (downhill direction)\n",
    "  alpha = 0.1\n",
    "  phi = None\n",
    "  return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFVfj7FR_iTF"
   },
   "source": [
    "Finally, let's run gradient descent and draw the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S34b5Fgl_jzA"
   },
   "outputs": [],
   "source": [
    "n_steps = 41\n",
    "phi_all = np.zeros((2,n_steps+1))\n",
    "phi_all[0,0] = 2.6\n",
    "phi_all[1,0] = 8.5\n",
    "lambda_ = 0.2\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss =  compute_loss2(data[0,:], data[1,:], model, phi_all[:,0:1], lambda_)\n",
    "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
    "\n",
    "for c_step in range (n_steps):\n",
    "  # Do gradient descent step\n",
    "  phi_all[:,c_step+1:c_step+2] = gradient_descent_step2(phi_all[:,c_step:c_step+1],lambda_, data)\n",
    "  # Measure loss and draw model every 8th step\n",
    "  if c_step % 8 == 0:\n",
    "    loss =  compute_loss2(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2], lambda_)\n",
    "    draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
    "\n",
    "draw_loss_function_reg(data, model, lambda_, my_colormap, phi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8BG3wu__mpv"
   },
   "source": [
    "You should see that the gradient descent algorithm now finds the correct minimum. By applying a tiny bit of domain knowledge (the parameter phi0 tends to be near zero and the parameter phi1 tends to be near 12.5), we get a better solution. However, the cost is that this solution is slightly biased towards this prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVsFkCxF8VjM"
   },
   "source": [
    "### Part 1.c) Low regularization\n",
    "Let's experiment with different values of the regularization weight `lambda_`. In this experiment. Set `lambda_=0.01`. What happens when the regularization value is small. What happens when the regularization value is small. What happens to the final loss value when we add the regularization term? Does it go up?  Go down?  Stay the same? Record your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTUA2x_Ut-WJ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gac157P0-tz9"
   },
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj03emkQhBoB"
   },
   "source": [
    "### Part 1.d) High regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZJwVmONhUmG"
   },
   "source": [
    "Now set `lambda_=1.0`. What happens when the regularization value is large. What happens to the final loss value when we add the regularization term? Does it go up?  Go down?  Stay the same? Record your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7r0E8w8qTej"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9CC2OKHh9-z"
   },
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sKX-NpVrQjI"
   },
   "source": [
    "## Q2) Overfitting to MNIST\n",
    "\n",
    "In this question, we will train a ConvNet on the MNIST dataset. The model will contain many more parameters than data points. We will investigate how to address the overfitting problem with L2 regularization (weight decay)\n",
    "\n",
    "The MNISt dataset contains cropped images of handwritten digits from 0 to 9. Our task will be to classify the input image to the correct digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lTBYy0WrRHZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PG4VoLhoeW2y"
   },
   "source": [
    "You will want to make sure that you're connected to a GPU instance for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ubVzDJ1rSVX"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k23Qx2Wleb6O"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PscTimnDeacY"
   },
   "source": [
    "Your device should be `device(type='cuda')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YJn3SqxeirH"
   },
   "source": [
    "Now let's specify our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GfBdnuV6_eYW"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9esRJIKcemCi"
   },
   "source": [
    "And setup our transforms. The following code will take an input image, cast it to a tensor and normalize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdKZa_tvegwz"
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LknGCl6MeszS"
   },
   "source": [
    "Let's download the MNIST dataset and create our dataloader object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSbqy73VrooM"
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-19jTVne5Ly"
   },
   "source": [
    "Let's write some code to visualize one of the images from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJZPTRG3e2Cd"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset.data[0], cmap='gray')\n",
    "plt.title('%i' % train_dataset.targets[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnOgAdrXfNAl"
   },
   "source": [
    "Now let's plot a bunch of digits together so you get an idea of what the training data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQJhScSxfGPN"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGJ9net-kFyB"
   },
   "source": [
    "### Q2.a) Training a ConvNet on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpwU4POBfVom"
   },
   "source": [
    "**TODO**: Here we will specify our CNN model. You will need to create a model class, called `CNN`, which inherits from the `nn.Module` base class.\n",
    "\n",
    "The CNN should have the following components:\n",
    "\n",
    "\n",
    "*   A `Conv2D` layer with 1 input channel, 32 output channels, a kernel size of 2 a stride of 1, and a padding of 3 pixels\n",
    "*   After the first `Conv2D` layer you should apply a `ReLU` acvtivation\n",
    "*   Then apply `MaxPool2d` with a kernel size of 2\n",
    "*   Next, you'll apply a second `Conv2D` layer with input channel of 32, output channel of 64, kernel size of 3, stride of 1, and padding of 2\n",
    "*   Apply another `ReLU` activation\n",
    "*   Introduce another `MaxPool2d` again with a kernel size of 2\n",
    "*   Next, you will flatten the output of the maxpool layer into a tensor with dimensions `batch_size x (64 * 9 * 9)`, where 64 is the number of output channgel from the previous `Conv2D` layer and 9 is the remaining spatial dimensions after applying maxpooling.\n",
    "*   Next, introduce a linear layer that maps from `(64 * 9 * 9)` to 1000\n",
    "*   Finally, apply a final, linear layer that maps from `1000` hidden units to `10` the number of classes in the dataset\n",
    "*   Note: we do not need to apply the softmax operation because the `nn.CrossEntropyLoss()` module requires unscaled logits for numerical stability. We will apply the argmax function in the test loop to evaluate accuracy.\n",
    "*   If you have implemented your model correctly, you should have 5,213,666 total parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWqEVzEQAKL6"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # TODO: Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Your code here\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdqu_ffUir79"
   },
   "source": [
    "Let's instantiate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmPVQG9iALi5"
   },
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enDORP2RiwRi"
   },
   "source": [
    "And count the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxtdrUeRsgJf"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "num_params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s88AmZ0Kiy6n"
   },
   "source": [
    "We've implemented a test to make sure you've implemented your model correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47GdyAFIiGjY"
   },
   "outputs": [],
   "source": [
    "assert num_params == 5213666, f\"Expected 5213666, got {num_params} parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsKhHFhriOjG"
   },
   "outputs": [],
   "source": [
    "print(f'Total number of parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIuCYqS_skfS"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of training examples: {len(train_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpbwFntkjA1W"
   },
   "source": [
    "**TODO**: Compute the ratio of parameters to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaDZUZoBjHC8"
   },
   "outputs": [],
   "source": [
    "# TODO: Fill this line here\n",
    "ratio =  None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfNOH0D8jUeN"
   },
   "outputs": [],
   "source": [
    "print(\"We have {:.2f}x more parameters than data points\".format(ratio)!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGFfEa08jidx"
   },
   "source": [
    "**TODO**: Write 1-2 sentences explaining why this is difficult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUSD4-yWjozG"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VE5rYtfjqd0"
   },
   "source": [
    "Let's instantiate our optimizer and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uF8aF3eiuc6"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lukWTuH0juit"
   },
   "source": [
    "Below is our training and test loop. Let's train our model to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Svnh9WW1rr90"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "trn_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch: {epoch + 1}\")\n",
    "    epoch_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        pbar.set_postfix({'loss': f'{loss.detach().item():.4f}', 'iteration': i+1})\n",
    "    epoch_loss /= len(train_loader)\n",
    "    trn_losses.append(epoch_loss)\n",
    "\n",
    "    print(\"Starting test set loop\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_test_loss = 0\n",
    "        for images, labels in tqdm(test_loader, total=len(test_loader)):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            total_test_loss += test_loss.detach().item()\n",
    "        total_test_loss /= len(test_loader)\n",
    "        print(f\"Test Loss: {total_test_loss:.4f}, Test: Accuracy: {(correct/total):.4f}\")\n",
    "        test_losses.append(total_test_loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NQPNIdGj1qt"
   },
   "source": [
    "We are all done training. Let's visualize our train and test loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vC0IGNQ9-jwA"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(np.arange(1, num_epochs+1), test_losses, linestyle='--', marker='o', label='TEST')\n",
    "plt.plot(np.arange(1, num_epochs+1), trn_losses, linestyle='--', marker='o', label='TRAIN')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_UimKeij6I2"
   },
   "source": [
    "**TODO:** What do you notice about the chart above? Is there a difference in the behavior of the train and test curves? If so, what is the difference and why do you think this is occuring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C04ihax3kCPn"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEEkYxYYSaK-"
   },
   "source": [
    "### Q2.b) Weight Decay\n",
    "\n",
    "Now we apply L2 regularization (often called weight decay) to our training algorithm. Fortunately, this is quite easy with the `Adam` optimizer class in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Cyr3KgtlM_J"
   },
   "source": [
    "**TODO**: Create a variable called `weight_decay` and set it to 0.001. Pass this into the `Adam` constructor and retrain your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_u2GKHr4StCL"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25\n",
    "# TODO: add your weight decay variable here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idxECNiSSqLh"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKZWr3Nrk0dd"
   },
   "source": [
    "**TODO**: Instantiate your optimizer object with weight decay. See the docs if you have any question: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0abT7enkyPm"
   },
   "outputs": [],
   "source": [
    "# TODO: implement an adam optimizer with weight decay\n",
    "optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwpkKCeplX7y"
   },
   "source": [
    "**TODO**: train your model with the same setup as before. However, this time create a new list of epoch-wise loss values for the train and the test loop. In other words, rename `trn_losses` --> `wd_train_losses` and test_losses --> `wd_test_losses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQHLs6_wSe4Q"
   },
   "outputs": [],
   "source": [
    "# TODO: Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOl4MrFClr5j"
   },
   "source": [
    "Now let's plot the epoch-wise losses from our baseline experiment (with no weight decay) and the epoch-wise losses from our training run with weight decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Bciifa0TM3-"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, num_epochs+1), test_losses, linestyle='--', marker='o', label='TEST')\n",
    "plt.plot(np.arange(1, num_epochs+1), trn_losses, linestyle='--', marker='o', label='TRAIN')\n",
    "\n",
    "plt.plot(np.arange(1, num_epochs+1), wd_test_losses, linestyle='--', marker='o', label='TEST (weight decay)')\n",
    "plt.plot(np.arange(1, num_epochs+1), wd_trn_losses, linestyle='--', marker='o', label='TRAIN (weight decay)')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZafbTMyc4Jf"
   },
   "source": [
    "**TODO**: Discuss your observations about 1) the behavior of the train loss in the baseline run vs the train loss in the weight decay run and 2) the behavior of the test loss in the baseline run vs the test loss of the weight decay run. Discuss whether or not you think applying weight decay was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6XOzxyamG3u"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRtf7A-spCsX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
